{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ba0834-ceaf-4c5d-aaf3-66d25982ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"cn.pdf\"\n",
    "\n",
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def text_formatter(text) -> str:\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def open_pdf(pdf_path) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_with_text = []\n",
    "    for pgNum, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text)\n",
    "        pages_with_text.append({\n",
    "            \"page_number\": pgNum+1,\n",
    "            \"page_char_count\": len(text),\n",
    "            \"page_word_count\": len(text.split(\" \")),\n",
    "            \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "            \"page_token_count\": len(text)/4,\n",
    "            \"text\": text\n",
    "        })\n",
    "    return pages_with_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b0ce30-b00e-4b68-a3b5-2fc93b0d5109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79a4db65abe4e799dc8c2a7c2759da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pages_with_text = open_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c27343-77d5-455a-b209-0edf4cc7c315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1753</td>\n",
       "      <td>250</td>\n",
       "      <td>27</td>\n",
       "      <td>438.25</td>\n",
       "      <td>Don't forget to check out the Online Learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8.50</td>\n",
       "      <td>DATA COMMUNICATIONS AND NETWORKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>175</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>43.75</td>\n",
       "      <td>McGraw-Hill Forouzan Networking Series Titles ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            1                0                1                        1   \n",
       "1            2             1753              250                       27   \n",
       "2            3                0                1                        1   \n",
       "3            4               34                4                        1   \n",
       "4            5              175               22                        2   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0              0.00                                                     \n",
       "1            438.25  Don't forget to check out the Online Learning ...  \n",
       "2              0.00                                                     \n",
       "3              8.50                 DATA COMMUNICATIONS AND NETWORKING  \n",
       "4             43.75  McGraw-Hill Forouzan Networking Series Titles ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_with_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58d7bb0-97cb-4991-9087-f7ac5b42cb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1171.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>586.0</td>\n",
       "      <td>1912.6</td>\n",
       "      <td>321.8</td>\n",
       "      <td>19.5</td>\n",
       "      <td>478.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>338.2</td>\n",
       "      <td>710.5</td>\n",
       "      <td>118.7</td>\n",
       "      <td>9.6</td>\n",
       "      <td>177.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>293.5</td>\n",
       "      <td>1468.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>586.0</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>482.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>878.5</td>\n",
       "      <td>2345.5</td>\n",
       "      <td>402.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>586.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1171.0</td>\n",
       "      <td>4026.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1006.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count       1171.0           1171.0           1171.0                   1171.0   \n",
       "mean         586.0           1912.6            321.8                     19.5   \n",
       "std          338.2            710.5            118.7                      9.6   \n",
       "min            1.0              0.0              1.0                      1.0   \n",
       "25%          293.5           1468.0            249.0                     13.0   \n",
       "50%          586.0           1929.0            326.0                     19.0   \n",
       "75%          878.5           2345.5            402.0                     25.0   \n",
       "max         1171.0           4026.0            694.0                     68.0   \n",
       "\n",
       "       page_token_count  \n",
       "count            1171.0  \n",
       "mean              478.2  \n",
       "std               177.6  \n",
       "min                 0.0  \n",
       "25%               367.0  \n",
       "50%               482.2  \n",
       "75%               586.4  \n",
       "max              1006.5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc86a5e-57f6-458e-8683-2073f1e61584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1ec3acb9c50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b432f834-baaf-45d7-b02c-aeb5dafb4046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa00aca31e6e4619bfb57874054a5ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_with_text):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3e4194-dcf8-4e46-bbf8-62b7520fa337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1171.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>586.0</td>\n",
       "      <td>1912.6</td>\n",
       "      <td>321.8</td>\n",
       "      <td>19.5</td>\n",
       "      <td>478.2</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>338.2</td>\n",
       "      <td>710.5</td>\n",
       "      <td>118.7</td>\n",
       "      <td>9.6</td>\n",
       "      <td>177.6</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>293.5</td>\n",
       "      <td>1468.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>586.0</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>482.2</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>878.5</td>\n",
       "      <td>2345.5</td>\n",
       "      <td>402.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>586.4</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1171.0</td>\n",
       "      <td>4026.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1006.5</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count       1171.0           1171.0           1171.0                   1171.0   \n",
       "mean         586.0           1912.6            321.8                     19.5   \n",
       "std          338.2            710.5            118.7                      9.6   \n",
       "min            1.0              0.0              1.0                      1.0   \n",
       "25%          293.5           1468.0            249.0                     13.0   \n",
       "50%          586.0           1929.0            326.0                     19.0   \n",
       "75%          878.5           2345.5            402.0                     25.0   \n",
       "max         1171.0           4026.0            694.0                     68.0   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count            1171.0                     1171.0  \n",
       "mean              478.2                       18.9  \n",
       "std               177.6                        9.8  \n",
       "min                 0.0                        0.0  \n",
       "25%               367.0                       13.0  \n",
       "50%               482.2                       18.0  \n",
       "75%               586.4                       25.0  \n",
       "max              1006.5                       58.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_with_text)\n",
    "df.describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c871647-c289-4e82-97cd-42128b638d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10\n",
    "\n",
    "def split_list(input_list,slice_size = chunk_size):\n",
    "    return [input_list[i:i + slice_size] for i in range(0,len(input_list), slice_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bcda2d-8dcd-4ea6-8957-5aef47fb591c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc6316f424e46178df4473345cf48a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_with_text):\n",
    "    item[\"sentence_chunks\"] = split_list(item[\"sentences\"])\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af65a6fa-4805-4ee5-b0ed-21ef15b99a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1171.000000</td>\n",
       "      <td>1171.000000</td>\n",
       "      <td>1171.000000</td>\n",
       "      <td>1171.000000</td>\n",
       "      <td>1171.000000</td>\n",
       "      <td>1171.000000</td>\n",
       "      <td>1171.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>586.000000</td>\n",
       "      <td>1912.625961</td>\n",
       "      <td>321.845431</td>\n",
       "      <td>19.487617</td>\n",
       "      <td>478.156490</td>\n",
       "      <td>18.875320</td>\n",
       "      <td>2.345004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>338.182889</td>\n",
       "      <td>710.513911</td>\n",
       "      <td>118.704370</td>\n",
       "      <td>9.615184</td>\n",
       "      <td>177.628478</td>\n",
       "      <td>9.755025</td>\n",
       "      <td>1.007077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>293.500000</td>\n",
       "      <td>1468.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>586.000000</td>\n",
       "      <td>1929.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>482.250000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>878.500000</td>\n",
       "      <td>2345.500000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>586.375000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1171.000000</td>\n",
       "      <td>4026.000000</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>1006.500000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count  1171.000000      1171.000000      1171.000000              1171.000000   \n",
       "mean    586.000000      1912.625961       321.845431                19.487617   \n",
       "std     338.182889       710.513911       118.704370                 9.615184   \n",
       "min       1.000000         0.000000         1.000000                 1.000000   \n",
       "25%     293.500000      1468.000000       249.000000                13.000000   \n",
       "50%     586.000000      1929.000000       326.000000                19.000000   \n",
       "75%     878.500000      2345.500000       402.000000                25.000000   \n",
       "max    1171.000000      4026.000000       694.000000                68.000000   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy   num_chunks  \n",
       "count       1171.000000                1171.000000  1171.000000  \n",
       "mean         478.156490                  18.875320     2.345004  \n",
       "std          177.628478                   9.755025     1.007077  \n",
       "min            0.000000                   0.000000     0.000000  \n",
       "25%          367.000000                  13.000000     2.000000  \n",
       "50%          482.250000                  18.000000     2.000000  \n",
       "75%          586.375000                  25.000000     3.000000  \n",
       "max         1006.500000                  58.000000     6.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_with_text)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41fd533d-26bc-4bc7-8970-889b52bac188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f052e369298344b68418e62f4e59fad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2746"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_with_text):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1edddc0c-82a0-4840-94e6-07c90fded50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2746.000000</td>\n",
       "      <td>2746.000000</td>\n",
       "      <td>2746.000000</td>\n",
       "      <td>2746.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>595.020393</td>\n",
       "      <td>813.829570</td>\n",
       "      <td>136.033139</td>\n",
       "      <td>203.457393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>329.674913</td>\n",
       "      <td>452.823724</td>\n",
       "      <td>72.238827</td>\n",
       "      <td>113.205931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>310.250000</td>\n",
       "      <td>490.250000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>122.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>606.500000</td>\n",
       "      <td>844.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>211.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>879.000000</td>\n",
       "      <td>1070.750000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>267.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1171.000000</td>\n",
       "      <td>4004.000000</td>\n",
       "      <td>572.000000</td>\n",
       "      <td>1001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count  2746.000000       2746.000000       2746.000000        2746.000000\n",
       "mean    595.020393        813.829570        136.033139         203.457393\n",
       "std     329.674913        452.823724         72.238827         113.205931\n",
       "min       2.000000          3.000000          1.000000           0.750000\n",
       "25%     310.250000        490.250000         84.000000         122.562500\n",
       "50%     606.500000        844.000000        142.000000         211.000000\n",
       "75%     879.000000       1070.750000        179.000000         267.687500\n",
       "max    1171.000000       4004.000000        572.000000        1001.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7ed1274-5a48-4b6e-88fd-b882bbb0277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token_length = 30\n",
    "pages_and_chunks_useful = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0d624c9-de1a-4cbd-a98a-3c86dd3d3620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh\\Code\\simple-local-rag\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Harsh\\Code\\simple-local-rag\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f28b9d4-2ea8-48ef-8e1f-d17864435e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8d0a8834354819b451bae97d54bfce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10min 25s\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embedding_model.to(\"cuda\")\n",
    "\n",
    "for item in tqdm(pages_and_chunks_useful):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af85c3dc-f711-4708-8fca-0d5e91e15c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_useful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17ccbe47-aaf6-4179-9e80-ccb9a07b9465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 36s\n",
      "Wall time: 32.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0263, -0.0086, -0.0045,  ..., -0.0379,  0.0099, -0.0303],\n",
       "        [-0.0134, -0.1077, -0.0024,  ..., -0.0145,  0.0182, -0.0258],\n",
       "        [-0.0306, -0.0435, -0.0626,  ...,  0.0023, -0.0069, -0.0420],\n",
       "        ...,\n",
       "        [ 0.0060, -0.1129,  0.0275,  ..., -0.0322,  0.0018,  0.0284],\n",
       "        [ 0.0127, -0.0957, -0.0211,  ..., -0.0301, -0.0308, -0.0251],\n",
       "        [-0.0160, -0.0771,  0.0096,  ..., -0.0202, -0.0345, -0.0339]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Embed all texts in batches\n",
    "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "                                               batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
    "                                               convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
    "\n",
    "text_chunk_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec3a612b-60b4-405c-b834-a7bb5e98dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_embeddings_df = pd.DataFrame(pages_and_chunks_useful)\n",
    "chunks_embeddings_df.to_csv(f\"{pdf_path.split('.',2)[0]}_chunks_embeddings_df.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c33d990b-8207-4cfe-90a4-055d6e56c9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Don't forget to check out the Online Learning ...</td>\n",
       "      <td>815</td>\n",
       "      <td>109</td>\n",
       "      <td>203.75</td>\n",
       "      <td>[-0.0262903962, -0.00862952601, -0.00454190047...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Student Resources The student resources are av...</td>\n",
       "      <td>604</td>\n",
       "      <td>90</td>\n",
       "      <td>151.00</td>\n",
       "      <td>[-0.0133625735, -0.107663088, -0.00236697868, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Facilitate learning through practice and revie...</td>\n",
       "      <td>323</td>\n",
       "      <td>42</td>\n",
       "      <td>80.75</td>\n",
       "      <td>[-0.0306028742, -0.0435124524, -0.0625793859, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>McGraw-Hill Forouzan Networking Series Titles ...</td>\n",
       "      <td>175</td>\n",
       "      <td>22</td>\n",
       "      <td>43.75</td>\n",
       "      <td>[-0.0135988621, -0.0270737838, 0.00158670451, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>DATA COMMUNICATIONS AND NETWORKING Fourth Edit...</td>\n",
       "      <td>347</td>\n",
       "      <td>52</td>\n",
       "      <td>86.75</td>\n",
       "      <td>[-0.0180227552, -0.00187136105, -0.0370334871,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>1169</td>\n",
       "      <td>SeeTCP transmission impairment, 80,88 transmis...</td>\n",
       "      <td>380</td>\n",
       "      <td>51</td>\n",
       "      <td>95.00</td>\n",
       "      <td>[-0.0141950902, -0.11565724, -0.00269556325, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>1170</td>\n",
       "      <td>real-time traffic, 916 reassembly, 38 responsi...</td>\n",
       "      <td>3479</td>\n",
       "      <td>500</td>\n",
       "      <td>869.75</td>\n",
       "      <td>[-0.0161629748, -0.0557909757, -0.0145120407, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>1170</td>\n",
       "      <td>SeeVDSL very low frequency. See VLF VHF,204 vi...</td>\n",
       "      <td>127</td>\n",
       "      <td>19</td>\n",
       "      <td>31.75</td>\n",
       "      <td>[0.00600702642, -0.112868801, 0.027525818, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>1171</td>\n",
       "      <td>1134 INDEX virtual circuit IntServ, 781 virtua...</td>\n",
       "      <td>1857</td>\n",
       "      <td>266</td>\n",
       "      <td>464.25</td>\n",
       "      <td>[0.0126872491, -0.0957111642, -0.0211406052, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>1171</td>\n",
       "      <td>SeeWATS wide-band CDMA, 478 wildcard filter st...</td>\n",
       "      <td>754</td>\n",
       "      <td>119</td>\n",
       "      <td>188.50</td>\n",
       "      <td>[-0.0160381403, -0.0770980939, 0.00961345434, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2642 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_number                                     sentence_chunk  \\\n",
       "0               2  Don't forget to check out the Online Learning ...   \n",
       "1               2  Student Resources The student resources are av...   \n",
       "2               2  Facilitate learning through practice and revie...   \n",
       "3               5  McGraw-Hill Forouzan Networking Series Titles ...   \n",
       "4               6  DATA COMMUNICATIONS AND NETWORKING Fourth Edit...   \n",
       "...           ...                                                ...   \n",
       "2637         1169  SeeTCP transmission impairment, 80,88 transmis...   \n",
       "2638         1170  real-time traffic, 916 reassembly, 38 responsi...   \n",
       "2639         1170  SeeVDSL very low frequency. See VLF VHF,204 vi...   \n",
       "2640         1171  1134 INDEX virtual circuit IntServ, 781 virtua...   \n",
       "2641         1171  SeeWATS wide-band CDMA, 478 wildcard filter st...   \n",
       "\n",
       "      chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0                  815               109             203.75   \n",
       "1                  604                90             151.00   \n",
       "2                  323                42              80.75   \n",
       "3                  175                22              43.75   \n",
       "4                  347                52              86.75   \n",
       "...                ...               ...                ...   \n",
       "2637               380                51              95.00   \n",
       "2638              3479               500             869.75   \n",
       "2639               127                19              31.75   \n",
       "2640              1857               266             464.25   \n",
       "2641               754               119             188.50   \n",
       "\n",
       "                                              embedding  \n",
       "0     [-0.0262903962, -0.00862952601, -0.00454190047...  \n",
       "1     [-0.0133625735, -0.107663088, -0.00236697868, ...  \n",
       "2     [-0.0306028742, -0.0435124524, -0.0625793859, ...  \n",
       "3     [-0.0135988621, -0.0270737838, 0.00158670451, ...  \n",
       "4     [-0.0180227552, -0.00187136105, -0.0370334871,...  \n",
       "...                                                 ...  \n",
       "2637  [-0.0141950902, -0.11565724, -0.00269556325, 0...  \n",
       "2638  [-0.0161629748, -0.0557909757, -0.0145120407, ...  \n",
       "2639  [0.00600702642, -0.112868801, 0.027525818, -0....  \n",
       "2640  [0.0126872491, -0.0957111642, -0.0211406052, 0...  \n",
       "2641  [-0.0160381403, -0.0770980939, 0.00961345434, ...  \n",
       "\n",
       "[2642 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "chunks_embeddings_df = pd.read_csv(f\"{pdf_path.split('.',2)[0]}_chunks_embeddings_df.csv\")\n",
    "chunks_embeddings_df[\"embedding\"] = chunks_embeddings_df[\"embedding\"].apply(lambda x : np.fromstring(x.strip(\"[]\"), sep = \"  \"))\n",
    "embeddings = torch.tensor(np.stack(chunks_embeddings_df[\"embedding\"].tolist(),axis=0),dtype=torch.float32).to(device)\n",
    "pages_and_chunks = chunks_embeddings_df.to_dict(orient=\"records\")\n",
    "\n",
    "chunks_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcfa123e-f82c-427c-b0f2-7f60167af46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh\\Code\\simple-local-rag\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f2f0837-3797-48a3-9fbe-0b39f0617ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 6 GB\n",
      "GPU memory: 6 | Recommended model: Gemma 2B in 4-bit precision.\n",
      "use_quantization_config set to: True\n",
      "model_id set to: google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")\n",
    "\n",
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54f31a10-1ae7-458c-96f7-687dbb176391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: sdpa\n",
      "[INFO] Using model_id: google/gemma-2b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa50d685a8c4335be2d82925be40b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "\n",
    "# 1. Create quantization config for smaller model loading (optional)\n",
    "# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n",
    "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Bonus: Setup Flash Attention 2 for faster inference, default to \"sdpa\" or \"scaled dot product attention\" if it's not available\n",
    "# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n",
    "# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
    "#model_id = \"google/gemma-7b-it\"\n",
    "model_id = model_id # (we already set this above)\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "\n",
    "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model) \n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False, # use full memory \n",
    "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
    "\n",
    "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU \n",
    "    llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fdc1d08-4dad-4036-9ff3-0a6eb874289a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae230c78-87fd-411d-848f-da73d7bcb660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1515268096"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0feb2c4c-12b9-4d89-aa70-26c006bac34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 2106740736, 'model_mem_mb': 2009.14, 'model_mem_gb': 1.96}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "620368cf-19c8-4688-887e-e76be481454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "re_rank_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-large-v1\")\n",
    "\n",
    "def get_ref(query,embedding_model,re_rank_model):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(device)\n",
    "\n",
    "    from time import perf_counter as timer\n",
    "    \n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "    end_time = timer()    \n",
    "    val, idx = torch.topk(dot_scores, k=5)\n",
    "    refs = []\n",
    "    for i in idx:\n",
    "        refs.append(pages_and_chunks[i.item()][\"sentence_chunk\"])\n",
    "    \n",
    "    re_ranked = re_rank_model.rank(query,refs,return_documents=True,top_k=3)\n",
    "    return re_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5612d67f-1520-4aec-a7a4-3c0ee3f0d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"text\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "Make sure your answers are as explanatory as possible.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e029479-be99-4c10-9bac-baa45747d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query,return_context=False):\n",
    "    # %%time\n",
    "    context_items = get_ref(query,embedding_model,re_rank_model)\n",
    "    prompt, context = prompt_formatter(query,context_items)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
    "                                 do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
    "                                 min_length=512, #minimum length of answer generated\n",
    "                                max_new_tokens=1024) # how many new tokens to generate from prompt \n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "    output = output_text.replace(prompt, '').replace(\"<bos>\",\"\").replace(\"<eos>\",\"\")\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    if return_context:\n",
    "        print(f\"Context:\\n {context}\\n\")\n",
    "    print(f\"RAG answer:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f8532eb-d44c-4125-b9b8-17c9c3d119df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain Bus and Ring topology\n",
      "\n",
      "Context:\n",
      " - In addition, a fault or break in the bus cable stops all transmission, even between devices on the same side of the problem. The damaged area reflects signals back in the direction of origin, creating noise in both directions. Bus topology was the one of the first topologies used in the design of early local- area networks. Ethernet LANs can use a bus topology, but they are less popular now for reasons we will discuss in Chapter 13. Ring Topology In a ring topology, each device has a dedicated point-to-point con- nection with only the two devices on either side of it. A signal is passed along the ring in one direction, from device to device, until it reaches its destination. Each device in the ring incorporates a repeater. When a device receives a signal intended for another device, its repeater regenerates the bits and passes them along (see Figure 1.8). Figure 1.8 A ring topology connecting six stations Repeater Repeater Repeater Repeater Repeater Repeater A ring is relatively easy to install and reconfigure. Each device is linked to only its immediate neighbors (either physically or logically).\n",
      "- The high-speed Token Ring networks called FDDI (Fiber Distributed Data Interface) and CDDI (Copper Distributed Data Interface) use this topology. In the bus ring topology, also called a token bus, the stations are connected to a sin- gle cable called a bus. They, however, make a logical ring, because each station knows the address of its successor (and also predecessor for token management purposes). When a station has finished sending its data, it releases the token and inserts the address of its successor in the token. Only the station with the address matching the destination address of the token gets the token to access the shared media. The Token Bus LAN, standardized by IEEE, uses this topology. In a star ring topology, the physical topology is a star. There is a hub, however, that acts as the connector. The wiring inside the hub makes the ring; the stations are con- nected to this ring through the two wire connections. This topology makes the network\n",
      "- SECTION 1.2 NETWORKS 13 Ring topology was prevalent when IBM introduced its local-area network Token Ring. Today, the need for higher-speed LANs has made this topology less popular. Hybrid Topology A network can be hybrid. For example, we can have a main star topol- ogy with each branch connecting several stations in a bus topology as shown in Figure 1.9. Figure 1.9 A hybrid topology: a star backbone with three bus networks Hub Network Models Computer networks are created by different entities. Standards are needed so that these heterogeneous networks can communicate with one another. The two best-known stan- dards are the OSI model and the Internet model. In Chapter 2 we discuss these two models. The OSI (Open Systems Interconnection) model defines a seven-layer net- work; the Internet model defines a five-layer network. This book is based on the Internet model with occasional references to the OSI model.\n",
      "\n",
      "RAG answer:\n",
      "Sure, here's a summary of the relevant passages from the context:\n",
      "\n",
      "**Bus topology:**\n",
      "\n",
      "- A bus topology is a simple topology where all devices are connected to a single central node.\n",
      "- Signals are transmitted in a single direction, and devices need to be physically close to the central node to communicate with it.\n",
      "- Bus topologies are easy to install and reconfigure, but they can be inefficient for high-speed communication.\n",
      "\n",
      "**Ring topology:**\n",
      "\n",
      "- A ring topology is a closed ring of nodes that are connected in a circular pattern.\n",
      "- Each node in the ring has a dedicated connection to the two nodes on either side.\n",
      "- Signals are transmitted in a continuous loop, and the destination node receives the signal regardless of its location.\n",
      "- Ring topologies are relatively easy to install and reconfigure, but they can be susceptible to failure if a node fails.\n"
     ]
    }
   ],
   "source": [
    "ask(\"Explain Bus and Ring topology\", return_context=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a961371-1d81-4036-aaea-9252c789a6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
